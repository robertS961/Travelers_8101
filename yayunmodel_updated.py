# -*- coding: utf-8 -*-
"""YayunModel_updated.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jUriKTs8uq8A_8ht6A4zrvxuceTnE4Rc
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.metrics import roc_auc_score, f1_score
from sklearn.ensemble import GradientBoostingClassifier
import warnings

warnings.filterwarnings('ignore')
out_dir = "figs"
os.makedirs(out_dir, exist_ok=True)
train = pd.read_csv('/content/drive/MyDrive/Travelers/Training_TriGuard.csv')
test = pd.read_csv('/content/drive/MyDrive/Travelers/Testing_TriGuard.csv')
target_col = "subrogation"
train = train.dropna(subset=[target_col]).reset_index(drop=True)
y = train[target_col]
X = train.drop(columns=[target_col])

for df in (X, test):
    df["claim_date"] = pd.to_datetime(df["claim_date"])
    df["claim_year"] = df["claim_date"].dt.year
    df["claim_month"] = df["claim_date"].dt.month
    df.drop(columns=["claim_date"], inplace=True)

    # Depreciation model parameters
    annual_rate = 0.12        # 12% per year
    mile_rate = 0.000002      # value loss per mile

    # Compute depreciated vehicle value


    if "year_of_born" in df.columns:
        invalid_yob = (df["year_of_born"] < 1900) | (df["year_of_born"] > df["claim_year"])
        df.loc[invalid_yob, "year_of_born"] = np.nan
        df["driver_age"] = df["claim_year"] - df["year_of_born"]
        invalid_age = (df["driver_age"] < 16) | (df["driver_age"] > 100)
        df.loc[invalid_age, "driver_age"] = np.nan
    if "vehicle_made_year" in df.columns:
        df["vehicle_age"] = (df["claim_year"] - df["vehicle_made_year"]).clip(0, 50)

    if "vehicle_price" in df.columns and "vehicle_mileage" in df.columns and "vehicle_age" in df.columns:
        df['vehicle_depr_value'] = (
            df['vehicle_price'] * (1 - annual_rate * df['vehicle_age']) *
            (1 - mile_rate * df['vehicle_mileage'])
        )
    if "claim_est_payout" in df.columns and "vehicle_depr_value" in df.columns:
        df["loss_ratio"] = (df["claim_est_payout"] / df["vehicle_depr_value"].replace(0, np.nan)).clip(0, 5)
    if "liab_prct" in df.columns:
        df['estimate_payout'] = (df['claim_est_payout'] - (.35) * df['vehicle_depr_value']) * (100 - df['liab_prct'])
    if "claim_est_payout" in df.columns:
        df["log_claim_payout"] = np.log1p(df["claim_est_payout"]) * df['liab_prct']
    '''
    df['driver_years'] = df['driver_age'] - df['age_of_DL']
    # normalize vectors

    df["years_norm"]   = (df["driver_years"] - df["driver_years"].min()) / (df["driver_years"].max() - df["driver_years"].min())
    df["safety_norm"]  = (df["safety_rating"] - df["safety_rating"].min()) / (df["safety_rating"].max() - df["safety_rating"].min())
    df["claims_norm"]  = (df["past_num_of_claims"] - df["past_num_of_claims"].min()) / (df["past_num_of_claims"].max() - df["past_num_of_claims"].min())

    # latent driver risk score (higher = more risky)
    df["driver_risk_latent"] = (
        0.4 * (1 - df["years_norm"]) +
        0.4 * (1 - df["safety_norm"]) +
        0.2 * df["claims_norm"]
    )
    df.drop(columns=["years_norm", "safety_norm", "claims_norm", 'driver_years'], inplace=True)
    '''

    df.drop(columns=["claim_year", "claim_month", 'vehicle_depr_value'], inplace=True)
    df.drop(columns = ["safety_rating", "annual_income", "past_num_of_claims", "vehicle_made_year",
                       "vehicle_price","vehicle_weight","age_of_DL","vehicle_mileage"
                       ])
    df.drop(columns=["year_of_born", "vehicle_color", "claim_day_of_week", "living_status",
                     "address_change_ind", "email_or_tel_available", "claim_est_payout"
                     ], inplace=True)




X_train, X_valid, y_train, y_valid = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

global_mean = y_train.mean()
tmp_train = X_train.copy()
tmp_train[target_col] = y_train.values
zip_stats = tmp_train.groupby("zip_code")[target_col].agg(["mean", "count"])
alpha = 20.0
zip_stats["zip_risk"] = (
        (zip_stats["mean"] * zip_stats["count"] + global_mean * alpha) /
        (zip_stats["count"] + alpha)
)

zip_risk_map = zip_stats["zip_risk"].to_dict()

for df in [X_train, X_valid, X, test]:
    df["zip_risk"] = df["zip_code"].map(zip_risk_map).fillna(global_mean)
    df.drop(columns=["zip_code"], inplace=True)

cat_cols = X_train.select_dtypes(include=["object"]).columns.tolist()
num_cols = [c for c in X_train.columns if c not in cat_cols and c != "claim_number"]

preprocessor = ColumnTransformer([
    ("num", SimpleImputer(strategy="median"), num_cols),
    ("cat", Pipeline([
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore"))
    ]), cat_cols),
])

preprocessor.fit(X_train)
X_train_proc = preprocessor.transform(X_train)
X_valid_proc = preprocessor.transform(X_valid)
X_test_proc = preprocessor.transform(test)
y_train_bin = y_train.astype(int)
y_valid_bin = y_valid.astype(int)

configs = {
    'config1_shallow': {
        "learning_rate": 0.05,
        "n_estimators": 400,
        "max_depth": 3,
        "subsample": 0.8,
        "max_features": 0.8,
        "random_state": 42
    },
    'config2_deeper': {
        "learning_rate": 0.05,
        "n_estimators": 600,
        "max_depth": 4,
        "subsample": 0.8,
        "max_features": 0.8,
        "min_samples_leaf": 30,
        "random_state": 42
    },
    'config3_regularized': {
        "learning_rate": 0.03,
        "n_estimators": 800,
        "max_depth": 3,
        "subsample": 0.7,
        "max_features": 0.7,
        "min_samples_leaf": 50,
        "random_state": 42
    }
}

best_f1 = 0
best_config_name = None
best_model = None
best_threshold = 0.5

for name, params in configs.items():
    print(f"\n  {name}:")
    model = GradientBoostingClassifier(**params)
    model.fit(X_train_proc, y_train_bin)
    valid_pred = model.predict_proba(X_valid_proc)[:, 1]
    auc = roc_auc_score(y_valid_bin, valid_pred)
    best_t = 0.5
    best_f1_config = 0.0
    for t in np.linspace(0.1, 0.9, 161):
        preds = (valid_pred >= t).astype(int)
        score = f1_score(y_valid_bin, preds)
        if score > best_f1_config:
            best_f1_config = score
            best_t = t

    print(f" AUC: {auc:.4f} | F1: {best_f1_config:.4f} @ threshold={best_t:.3f}")

    if best_f1_config > best_f1:
        best_f1 = best_f1_config
        best_config_name = name
        best_model = model
        best_threshold = best_t

print(f"BEST CONFIGURATION: {best_config_name}")
print(f"F1:  {best_f1:.4f}")
print(f"Best Threshold: {best_threshold:.3f}")

test_pred_proba = best_model.predict_proba(X_test_proc)[:, 1]
test_pred_label = (test_pred_proba >= best_threshold).astype(int)

submission = pd.DataFrame({
    "claim_number": test["claim_number"],
    "subrogation": test_pred_label
})

save_path = "submission_1208.csv"
submission.to_csv(save_path, index=False)

print(f"Saved: {save_path}")
print(f"  Test positive rate: {test_pred_label.mean():.4f}")
print(f"  LightGBM baseline:  Valid F1 ≈ 0.5690, Test ≈ 0.59385")
print(f"  Current model:  Valid F1 = {best_f1:.4f}")

"""Yayun model - Current model:  Valid F1 = 0.5809

Addition of Vehicle Deprication Value - model:  Valid F1 = 0.5770

Addition of Estimate Payout. Taking into account the estimated payout - depreciated car value(muliplied by salavage indictator) * libability percentage - model: Current model:  Valid F1 = 0.5790

Addition of the safety estimator - model :Valid F1 = 0.5787

Addition of the safety estimator but normalizated -model:  Valid F1 = 0.5764

Removable of unecessary variables for numerical - model : Valid F1 = 0.5769

Removedable of uncessary numerical and categorical variables: Current model:  Valid F1 = 0.5763

Removable of sub variables of the latent, numerical and categorical - Current model:  Valid F1 = 0.5764

Removeable of safety estimator, sub variables of latent , numerical , and categorical   Current model:  Valid F1 = 0.5803
"""

