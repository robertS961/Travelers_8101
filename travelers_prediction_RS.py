# -*- coding: utf-8 -*-
"""Travelers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E6PYXzrPIkIgNst7SdelE2G__PNFPreX
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Exploration of the Training Set



"""

!pip install pandas

import pandas as pd

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline

from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import f1_score

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import (
    RandomForestClassifier,
    GradientBoostingClassifier,
    AdaBoostClassifier,
    BaggingClassifier,
    ExtraTreesClassifier,
    HistGradientBoostingClassifier
)

# If you have these installed:
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

import pandas as pd

df = pd.read_csv('/content/drive/MyDrive/Travelers/Training_TriGuard.csv')
df_test = pd.read_csv('/content/drive/MyDrive/Travelers/Testing_TriGuard.csv')

df_sub = df[df.subrogation == 1]
df_nosub = df[df.subrogation == 0]

cols = df.columns
print(cols)

categorical_vars = [
    "gender",
    "email_or_tel_available",
    "high_education_ind",
    "address_change_ind",
    "living_status",
    "zip_code",
    "claim_day_of_week",
    "accident_site",
    "witness_present_ind",
    "channel",
    "policy_report_filed_ind",
    "vehicle_category",
    "vehicle_color",
    "accident_type",
    "in_network_bodyshop"
]

numerical_vars = [
    "year_of_born",
    "safety_rating",
    "annual_income",
    "past_num_of_claims",
    "liab_prct",
    "claim_est_payout",
    "vehicle_made_year",
    "vehicle_price",
    "vehicle_weight",
    "age_of_DL",
    "vehicle_mileage"
]

for col in categorical_vars:
    print("=" * 60)
    print(f"ðŸ”¹ Column: {col}\n")

    # Value counts (absolute counts)
    sub_counts = df_sub[col].value_counts(dropna=False)
    nosub_counts = df_nosub[col].value_counts(dropna=False)
    total_counts = df[col].value_counts(dropna=False)

    # Ratios (percentage of total)
    sub_ratio = sub_counts / len(df_sub)
    nosub_ratio = nosub_counts / len(df_nosub)
    total_ratio = total_counts / len(df)

    # Print results

    print("ðŸ“Œ Subrogation = 1")
    print(pd.DataFrame({"count": sub_counts, "ratio": sub_ratio.round(3)}))
    print()

    print("ðŸ“Œ Subrogation = 0")
    print(pd.DataFrame({"count": nosub_counts, "ratio": nosub_ratio.round(3)}))
    print("\n")

    print("ðŸ“Œ Total")
    print(pd.DataFrame({"count": total_counts, "ratio": total_ratio.round(3)}))
    print("=" * 60)
    print()

for col in numerical_vars:
    print("=" * 70)
    print(f"ðŸ”¹ Numerical Column: {col}\n")

    print("ðŸ“Œ Subrogation = 1 (df_sub)")
    print(df_sub[col].describe())
    print()

    print("ðŸ“Œ Subrogation = 0 (df_nosub)")
    print(df_nosub[col].describe())
    print()

    print("ðŸ“Œ Full Dataset (df)")
    print(df[col].describe())
    print("\n")

import matplotlib.pyplot as plt
import seaborn as sns

def plot_distribution(col):
    plt.figure(figsize=(12, 5))

    # KDE plot
    plt.subplot(1, 2, 1)
    sns.kdeplot(df_sub[col], label="Subrogation = 1", fill=True, alpha=0.5)
    sns.kdeplot(df_nosub[col], label="Subrogation = 0", fill=True, alpha=0.5)
    plt.title(f"KDE Plot: {col}")
    plt.legend()

    # Boxplot
    plt.subplot(1, 2, 2)
    sns.boxplot(data=df, x="subrogation", y=col)
    plt.title(f"Boxplot: {col}")

    plt.tight_layout()
    plt.show()

for col in numerical_vars:
    plot_distribution(col)

def plot_categorical(col):
    plt.figure(figsize=(10, 5))

    # proportion per category
    overall_ratio = df[col].value_counts(normalize=True)
    sub_ratio     = df_sub[col].value_counts(normalize=True)
    nosub_ratio   = df_nosub[col].value_counts(normalize=True)

    # Build a new DataFrame with groups on x-axis
    ratio_df = pd.DataFrame({
        "No Sub": nosub_ratio,
        "Sub": sub_ratio,
        "Total": overall_ratio
    }).fillna(0)

    # Transpose so x-axis = group names
    ratio_df = ratio_df.T   # rows: groups, columns: categories

    # Plot
    ratio_df.plot(kind="bar", figsize=(10, 5))
    plt.title(f"Category Proportions for: {col}")
    plt.ylabel("Proportion")
    plt.xticks(rotation=0)
    plt.legend(title="Category", bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    plt.show()

for col in categorical_vars:
    plot_categorical(col)

"""# Meeting 1, Dec 4

## Latent Variable Salavage

## Important Predictors
- Liability Pct, Witness Present, Accident Type, Accident Location

## Zip Code
- Mapping to dense travelers area
or Mapping- State/City density

## Important Numerical Columns

### Most Important
Liab-Pct

### Maybe Important
Vehicle Price, Past number of Claims
They had a different in peak spike

## Important Categorical Columns

### Most Important
Accident cite, Witness Present, Accident type

### Maybe Important
In network body shop, living status, higher education, gender

## Complications

## The firm size matters. Why someone with great stats get a 0 for subgagrate

### Crash into someone from their own insurance.
### Crash into a family or friend
### They decide to salvage

### Auto industries can also have alliances especially smaller firms.

### 2-3 percent of auto insurance is subrogation while 15 percent if subrogation and salvage.

### Examine the zip codes for zip codes where travels insurance is high concentrated. Make those subogration 0.

### Compound variable of vehicle price to annual income and est payout

### Net Cash- Estimated payout / car value

### compound variable of estimate payout, car price, and liability percentage

### Salvage variable: Car value to estimated payout

# Exploration of the Test Set
"""



import matplotlib.pyplot as plt
import seaborn as sns

def plot_distribution_test(col):
    plt.figure(figsize=(12, 5))

    # KDE plot
    plt.subplot(1, 2, 1)
    sns.kdeplot(df_test[col], label="Test_Data = 1", fill=True, alpha=0.5)
    sns.kdeplot(df[col], label="Test_Data = 0", fill=True, alpha=0.5)
    plt.title(f"KDE Plot: {col}")
    plt.legend()
    plt.show()

for col in numerical_vars:
    print("=" * 70)
    print(f"ðŸ”¹ Numerical Column: {col}\n")

    print("ðŸ“Œ Train Data (df_test)")
    print(df_test[col].describe())
    print()

    print("ðŸ“Œ Subrogation = 0 (df_train)")
    print(df[col].describe())
    print()

for col in numerical_vars:
    plot_distribution_test(col)

for col in categorical_vars:
    print("=" * 60)
    print(f"ðŸ”¹ Column: {col}\n")

    # Value counts (absolute counts)
    sub_counts = df_test[col].value_counts(dropna=False)
    total_counts = df[col].value_counts(dropna=False)

    # Ratios (percentage of total)
    test_ratio = sub_counts / len(df_test)
    total_ratio = total_counts / len(df)

    # Print results

    print("ðŸ“Œ Subrogation = 1")
    print(pd.DataFrame({"count": sub_counts, "ratio": test_ratio.round(3)}))
    print()


    print("ðŸ“Œ Total")
    print(pd.DataFrame({"count": total_counts, "ratio": total_ratio.round(3)}))
    print("=" * 60)
    print()

def plot_categorical_test(col):
    plt.figure(figsize=(10, 5))

    # proportion per category
    overall_ratio = df[col].value_counts(normalize=True)
    sub_ratio     = df_test[col].value_counts(normalize=True)

    # Build a new DataFrame with groups on x-axis
    ratio_df = pd.DataFrame({
        "Test": sub_ratio,
        "Total": overall_ratio
    }).fillna(0)

    # Transpose so x-axis = group names
    ratio_df = ratio_df.T   # rows: groups, columns: categories

    # Plot
    ratio_df.plot(kind="bar", figsize=(10, 5))
    plt.title(f"Category Proportions for: {col}")
    plt.ylabel("Proportion")
    plt.xticks(rotation=0)
    plt.legend(title="Category", bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    plt.show()

for col in categorical_vars:
    plot_categorical_test(col)

"""# Exploration of Interaction Variables

"""

df['pay_v_price'] = df['claim_est_payout'] - df['vehicle_price']
df['pay_v_price_ratio'] = df['claim_est_payout'] / df['vehicle_price']

df_sub = df[df['subrogation'] == 1].copy()
df_nosub = df[df['subrogation'] == 0].copy()


df_nosub['pay_v_price'] = df_nosub['vehicle_price'] - df_nosub['claim_est_payout']
df_nosub['pay_v_price_ratio'] = df_nosub['claim_est_payout'] / df_nosub['vehicle_price']
df_nosub['liab_v_price'] = (df_nosub['liab_prct']) * df_nosub['vehicle_price']
df_nosub['liab_v_pay_v_price'] = (df_nosub['liab_prct']) * df_nosub['pay_v_price']
df_nosub['liab_v_pay'] = (df_nosub['liab_prct']) * df_nosub['claim_est_payout']
df_nosub['liab_v_pay_price'] = (df_nosub['liab_prct']) * df_nosub['claim_est_payout'] * df_nosub['vehicle_price']
df_sub['pay_v_price'] = df_sub['vehicle_price'] - df_sub['claim_est_payout']
df_sub['pay_v_price_ratio'] = df_sub['claim_est_payout'] / df_sub['vehicle_price']
df_sub['liab_v_price'] = (df_sub['liab_prct']) * df_sub['vehicle_price']
df_sub['liab_v_pay_v_price'] = (df_sub['liab_prct']) * df_sub['pay_v_price']
df_sub['liab_v_pay'] = (df_sub['liab_prct']) * df_sub['claim_est_payout']
df_sub['liab_v_pay_price'] = (df_sub['liab_prct']) * df_sub['claim_est_payout'] * df_sub['vehicle_price']

new_cols = ['pay_v_price', 'pay_v_price_ratio', 'liab_v_price', 'liab_v_pay_v_price', 'liab_v_pay', 'liab_v_pay_price']



import matplotlib.pyplot as plt
import seaborn as sns

def plot_distribution_test(col):
    plt.figure(figsize=(12, 5))

    # KDE plot
    plt.subplot(1, 2, 1)
    sns.kdeplot(df_sub[col], label=" Subgogration= 1", fill=True, alpha=0.5)
    sns.kdeplot(df_nosub[col], label="Subgogration = 0", fill=True, alpha=0.5)
    plt.title(f"KDE Plot: {col}")
    plt.legend()
    plt.show()

for col in new_cols:
  plot_distribution_test(col)

"""# Baseline Code"""

df['subrogation'].info()
df['liab_v_price'] = df['liab_prct'] * df['vehicle_price']
df['liab_v_pay'] = df['liab_prct'] * df['claim_est_payout']
df['pay_v_price'] = df['vehicle_price'] - df['claim_est_payout']
df['liab_v_pay_v_price'] = df['liab_prct'] * df['pay_v_price']

df_test['liab_v_price'] = df_test['liab_prct'] * df_test['vehicle_price']
df_test['liab_v_pay'] = df_test['liab_prct'] * df_test['claim_est_payout']
df_test['pay_v_price'] = df_test['vehicle_price'] - df_test['claim_est_payout']
df_test['liab_v_pay_v_price'] = df_test['liab_prct'] * df_test['pay_v_price']

categorical_vars = [
    "gender",
    "email_or_tel_available",
    "high_education_ind",
    "address_change_ind",
    "living_status",
    "zip_code",
    "claim_day_of_week",
    "accident_site",
    "witness_present_ind",
    "channel",
    "policy_report_filed_ind",
    "vehicle_category",
    "vehicle_color",
    "accident_type",
    "in_network_bodyshop"
]

numerical_vars = [
    "year_of_born",
    "safety_rating",
    "annual_income",
    "past_num_of_claims",
    "liab_prct",
    "claim_est_payout",
    "vehicle_made_year",
    "vehicle_price",
    "vehicle_weight",
    "age_of_DL",
    "vehicle_mileage",
    'liab_v_price',
    'liab_v_pay',
    'liab_v_pay_v_price',
    'pay_v_price'
]
numerical_predictors_backup = ['liab_prct', 'liab_v_price', 'liab_v_pay', 'liab_v_pay_v_price']
numerical_predictors = ['liab_prct', 'liab_v_pay' ]
categorical_predictors = ['witness_present_ind', 'accident_site' , 'accident_type', "gender"]
predictors = numerical_predictors + categorical_predictors

"""## Produce Results with different Methods"""

random_state = 42
methods = {
    "Logistic Regression": LogisticRegression(
        max_iter=1000,
        class_weight="balanced"  # often helps with F1
    ),

    "Random Forest": RandomForestClassifier(
        n_estimators=300,
        max_depth=None,
        random_state=random_state,
        n_jobs=-1,
        class_weight="balanced_subsample"
    ),

    "Extra Trees": ExtraTreesClassifier(
        n_estimators=300,
        random_state=random_state,
        n_jobs=-1,
        class_weight="balanced_subsample"
    ),

    "Gradient Boosting": GradientBoostingClassifier(
        random_state=random_state
    ),

    "HistGradientBoosting": HistGradientBoostingClassifier(
        random_state=random_state
    ),

    "AdaBoost": AdaBoostClassifier(
        random_state=random_state
    ),

    "Bagging (LogReg base)": BaggingClassifier(
        estimator=LogisticRegression(max_iter=1000),
        n_estimators=50,
        random_state=random_state,
        n_jobs=-1
    ),

    "XGBoost": XGBClassifier(
        n_estimators=300,
        learning_rate=0.05,
        max_depth=4,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=random_state,
        objective="binary:logistic",
        eval_metric="logloss",
        n_jobs=-1
    ),

    "LightGBM": LGBMClassifier(
        n_estimators=300,
        learning_rate=0.05,
        max_depth=-1,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=random_state,
        objective="binary",
        n_jobs=-1
    ),
}

def produce_results(
    df,
    numerical_predictors,
    categorical_predictors,
    methods=None,
    target='subrogation',
    test_size=0.50,
    random_state=42,
    cv_splits=5
):
    fitted_models = {}

    # Predictor list
    predictors = numerical_predictors + categorical_predictors

    # Remove rows with missing values in predictors or target
    df_clean = df.dropna(subset=predictors + [target]).copy()

    X = df_clean[predictors]
    y = df_clean[target]

    # Train/test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y,
        test_size=test_size,
        random_state=random_state,
        stratify=y
    )

    # Shared preprocessing
    preprocess = ColumnTransformer(
        transformers=[
            ("num", StandardScaler(), numerical_predictors),
            ("cat", OneHotEncoder(handle_unknown='ignore'), categorical_predictors)
        ]
    )

    # 5-fold stratified CV
    cv = StratifiedKFold(
        n_splits=cv_splits,
        shuffle=True,
        random_state=random_state
    )

    results = []


    # Loop through all ML methods
    for name, clf in methods.items():
        print("=" * 70)
        print(f"Model: {name}")

        # Pipeline: preprocessing â†’ model
        model = Pipeline(steps=[
            ("preprocess", preprocess),
            ("clf", clf)
        ])

        # Cross-validation on training data using F1 Macro
        cv_scores = cross_val_score(
            model,
            X_train,
            y_train,
            cv=cv,
            scoring='f1'
        )

        print(f"CV F1  scores: {cv_scores}")
        print(f"Mean CV F1: {cv_scores.mean():.3f}")

        # Fit on full training data
        model.fit(X_train, y_train)




        y_pred = model.predict(X_test)

        # Test F1 Macro
        test_f1 = f1_score(y_test, y_pred, average='binary', pos_label=1)

        print(f"Test F1 : {test_f1:.3f}\n")

        # Save results
        results.append({
            "model_name": name,
            "cv_mean_f1": cv_scores.mean(),
            "cv_std_f1": cv_scores.std(),
            "test_f1": test_f1
        })
        fitted_models[name] = model


    # Return a sorted summary table
    results_df = pd.DataFrame(results).sort_values(
        by="test_f1",
        ascending=False
    )

    print("\n=== Summary (sorted by Test F1) ===")
    print(results_df)

    return results_df, fitted_models

results, fitted_models = produce_results(df, numerical_predictors, categorical_predictors, methods)
results

"""## Find Important Predictions Based on Models"""

rf_model = fitted_models["Random Forest"]   # this is a Pipeline
clf = rf_model.named_steps["clf"]  # the RandomForestClassifier inside the pipeline
importances = clf.feature_importances_

preprocess = rf_model.named_steps["preprocess"]

# numeric features (unchanged)
num_features = numerical_predictors

# categorical features (expanded by OneHotEncoder)
ohe = preprocess.named_transformers_["cat"]
cat_features = ohe.get_feature_names_out(categorical_predictors)

all_feature_names = list(num_features) + list(cat_features)

feat_importance_df = pd.DataFrame({
    "feature": all_feature_names,
    "importance": importances
}).sort_values("importance", ascending=False)

feat_importance_df.head(20)

"""## Find Logistic Regression Boundary Line

"""

predictors = numerical_predictors + categorical_predictors
target='subrogation'
# Remove rows with missing values in predictors or target
df_clean = df.dropna(subset=predictors + [target]).copy()

X = df_clean[predictors]
y = df_clean[target]

X_train, X_test, y_train, y_test = train_test_split(
        X, y,
        test_size=0.5,
        random_state=random_state,
        stratify=y
)

log_reg_model = fitted_models["Logistic Regression"]  # adjust key if different


y_proba = log_reg_model.predict_proba(X_test)[:, 1]  # probability of class 1

# 3) Sweep thresholds and find the one that maximizes plain F1 (binary)
thresholds = np.linspace(0.05, 0.95, 1800)
best_thresh = 0.5
best_f1 = 0.0

for t in thresholds:
    y_pred_thresh = (y_proba >= t).astype(int)  # convert to 0/1 ints
    f1 = f1_score(y_test, y_pred_thresh)        # plain binary F1 (pos_label=1)
    if f1 > best_f1:
        best_f1 = f1
        best_thresh = t

print(f"Best threshold: {best_thresh:.3f}")
print(f"Best F1 (binary) at that threshold: {best_f1:.4f}")

"""## Tune up Logistic Regression"""

# Not using this yet, still playing around with it. Could increase our accuracy a little

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.metrics import f1_score


def tune_logistic_regression(
    df,
    numerical_predictors,
    categorical_predictors,
    target='subrogation',
    test_size=0.5,
    random_state=42,
    cv_splits=5
):
    predictors = numerical_predictors + categorical_predictors

    # Drop rows with NA in predictors or target
    df_clean = df.dropna(subset=predictors + [target]).copy()

    X = df_clean[predictors]
    y = df_clean[target]

    # Train/test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y,
        test_size=test_size,
        random_state=random_state,
        stratify=y
    )

    # Preprocess: scale numeric, OHE categorical
    preprocess = ColumnTransformer(
        transformers=[
            ("num", StandardScaler(), numerical_predictors),
            ("cat", OneHotEncoder(handle_unknown='ignore'), categorical_predictors)
        ]
    )

    base_clf = LogisticRegression(
        max_iter=1000,
        class_weight="balanced"  # often helps with F1
    )

    pipe = Pipeline(steps=[
        ("preprocess", preprocess),
        ("clf", base_clf)
    ])

    # Hyperparameter grid for Logistic Regression
    param_grid = {
        "clf__C": [0.01, 0.1, 1, 10, 100],
        "clf__class_weight": [None, "balanced"],
        "clf__solver": ["lbfgs", "liblinear"],  # common robust solvers
        # penalty 'l2' is default; liblinear can also use 'l1' if you want:
        # "clf__penalty": ["l2", "l1"],
    }

    cv = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=random_state)

    grid = GridSearchCV(
        pipe,
        param_grid=param_grid,
        scoring="f1",
        cv=cv,
        n_jobs=-1
    )

    print("Running GridSearchCV for Logistic Regression...")
    grid.fit(X_train, y_train)

    print("\nBest params:", grid.best_params_)
    print("Best CV F1 :", grid.best_score_)

    # Evaluate best model on held-out test set
    best_model = grid.best_estimator_
    y_pred = best_model.predict(X_test)
    test_f1 = f1_score(y_test, y_pred, average='binary', pos_label=1)
    print("Test F1 (best Logistic Regression):", test_f1)


    return best_model, test_f1

tune_logistic_regression(df, numerical_predictors, categorical_predictors)

"""## Produce Output for Kaggle"""

y_pred = fitted_models['Logistic Regression'].predict(df_test[predictors]).astype(int)
df_test['subrogation'] = y_pred
output = df_test[['claim_number', 'subrogation']]
output
output.to_csv("travelers_predictions.csv", index=False)

logreg_pipe = fitted_models['Logistic Regression']
best_threshold = best_thresh   # or whatever value you found, e.g. 0.37

# Predicted probability for subrogation = 1
probs_submit = logreg_pipe.predict_proba(df_test[predictors])[:, 1]

# Apply custom threshold and force int 0/1
y_pred = (probs_submit >= best_threshold).astype(int)

df_test['subrogation'] = y_pred

output = df_test[['claim_number', 'subrogation']]
output.to_csv("travelers_predictions_01.csv", index=False)